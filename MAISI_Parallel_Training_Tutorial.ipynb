{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ca0ac4",
   "metadata": {},
   "source": [
    "# MAISI 3D Diffusion Model - Parallel Training Tutorial\n",
    "\n",
    "## üöÄ Complete Guide to Multi-GPU Medical Image Generation\n",
    "\n",
    "This tutorial provides a step-by-step guide to set up, configure, and run the MAISI parallel training script for generating 3D medical images. We'll cover everything from environment setup to troubleshooting common errors.\n",
    "\n",
    "### What You'll Learn:\n",
    "- ‚úÖ **Environment Setup**: Install dependencies and verify GPU availability\n",
    "- ‚úÖ **Data Preparation**: Properly configure medical imaging data paths  \n",
    "- ‚úÖ **Parallel Training**: Run multi-GPU training with torchrun\n",
    "- ‚úÖ **Error Resolution**: Fix common issues like missing data paths\n",
    "- ‚úÖ **Output Analysis**: Understand generated images and file structure\n",
    "\n",
    "### Prerequisites:\n",
    "- NVIDIA GPU(s) with CUDA support\n",
    "- Python 3.8+ environment\n",
    "- Access to medical imaging data (.nii.gz files) or willingness to use simulated data\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Error Analysis from Your Previous Run\n",
    "\n",
    "Your command failed because you used the placeholder path `/path/to/medical/data`. Let's fix this step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af8251",
   "metadata": {},
   "source": [
    "# 1. Check Environment and Install Dependencies\n",
    "\n",
    "Let's start by verifying your Python environment and installing the required packages for MAISI training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b31c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîç ENVIRONMENT CHECK:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if we're in a virtual environment\n",
    "if hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):\n",
    "    print(\"‚úÖ Running in virtual environment\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Not in virtual environment - consider using one\")\n",
    "\n",
    "print(\"\\nüì¶ INSTALLING REQUIRED PACKAGES:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Install required packages\n",
    "packages = [\n",
    "    \"torch\",\n",
    "    \"torchvision\", \n",
    "    \"monai-weekly[pillow,tqdm]\",\n",
    "    \"nibabel\",\n",
    "    \"numpy\",\n",
    "    \"scipy\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "        print(f\"‚úÖ {package} installed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Failed to install {package}: {e}\")\n",
    "\n",
    "print(\"\\nüéâ Installation complete!\")\n",
    "print(\"Next: Let's check GPU availability...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac99c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üñ•Ô∏è  GPU AVAILABILITY CHECK:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"‚úÖ CUDA is available! Found {gpu_count} GPU(s)\")\n",
    "        \n",
    "        for i in range(gpu_count):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "            print(f\"   GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "            \n",
    "        # Test GPU functionality\n",
    "        print(f\"\\nüß™ Testing GPU {0}...\")\n",
    "        x = torch.randn(1000, 1000).cuda()\n",
    "        y = torch.matmul(x, x.T)\n",
    "        print(f\"‚úÖ GPU computation test passed!\")\n",
    "        del x, y  # Free memory\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå CUDA not available. Training will be slow on CPU.\")\n",
    "        print(\"   Make sure you have:\")\n",
    "        print(\"   ‚Ä¢ NVIDIA GPU installed\")\n",
    "        print(\"   ‚Ä¢ CUDA drivers installed\") \n",
    "        print(\"   ‚Ä¢ PyTorch with CUDA support\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ùå PyTorch not available. Please install it first.\")\n",
    "    \n",
    "print(f\"\\nüìä SYSTEM RESOURCES:\")\n",
    "print(\"=\" * 50)\n",
    "try:\n",
    "    import psutil\n",
    "    print(f\"CPU cores: {psutil.cpu_count()}\")\n",
    "    print(f\"RAM: {psutil.virtual_memory().total / 1024**3:.1f} GB\")\n",
    "    print(f\"Available RAM: {psutil.virtual_memory().available / 1024**3:.1f} GB\")\n",
    "except ImportError:\n",
    "    print(\"psutil not available for system info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79c9ffa",
   "metadata": {},
   "source": [
    "# 2. Verify Data Directory and File Presence\n",
    "\n",
    "**This is where your previous command failed!** The path `/path/to/medical/data` was just a placeholder. Let's set up the correct data path for your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64177ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üìÅ DATA DIRECTORY CONFIGURATION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Option 1: Use real medical data (if you have it)\n",
    "# CHANGE THIS PATH to your actual medical data directory!\n",
    "REAL_DATA_PATH = \"/home/santino/medical_data\"  # ‚¨ÖÔ∏è UPDATE THIS PATH!\n",
    "\n",
    "# Option 2: Common paths where medical data might be stored\n",
    "common_paths = [\n",
    "    \"/home/santino/medical_data\",\n",
    "    \"/home/santino/data/medical\",\n",
    "    \"/data/medical\",\n",
    "    \"/mnt/medical_data\",\n",
    "    \"/shared/medical_data\",\n",
    "    \"~/Documents/medical_data\",\n",
    "    \"~/Desktop/medical_data\"\n",
    "]\n",
    "\n",
    "print(\"üîç Searching for medical data in common locations...\")\n",
    "\n",
    "found_data = False\n",
    "data_files = []\n",
    "\n",
    "for path in common_paths:\n",
    "    expanded_path = os.path.expanduser(path)\n",
    "    if os.path.exists(expanded_path):\n",
    "        nii_files = glob.glob(os.path.join(expanded_path, \"*.nii.gz\"))\n",
    "        if nii_files:\n",
    "            print(f\"‚úÖ Found {len(nii_files)} .nii.gz files in: {expanded_path}\")\n",
    "            REAL_DATA_PATH = expanded_path\n",
    "            data_files = nii_files\n",
    "            found_data = True\n",
    "            break\n",
    "        else:\n",
    "            print(f\"üìÇ Directory exists but no .nii.gz files: {expanded_path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Directory not found: {expanded_path}\")\n",
    "\n",
    "if not found_data:\n",
    "    print(f\"\\n‚ö†Ô∏è  NO MEDICAL DATA FOUND!\")\n",
    "    print(f\"Please either:\")\n",
    "    print(f\"1. Place your .nii.gz files in one of these directories:\")\n",
    "    for path in common_paths[:3]:\n",
    "        print(f\"   ‚Ä¢ {path}\")\n",
    "    print(f\"2. Or update REAL_DATA_PATH variable above with your actual path\")\n",
    "    print(f\"3. Or use simulated data for testing (shown below)\")\n",
    "    \n",
    "    # Set flag to use simulated data\n",
    "    USE_SIMULATED_DATA = True\n",
    "    print(f\"\\nüé≠ Will use SIMULATED DATA for this demo\")\n",
    "else:\n",
    "    USE_SIMULATED_DATA = False\n",
    "    print(f\"\\n‚úÖ Will use REAL MEDICAL DATA from: {REAL_DATA_PATH}\")\n",
    "    print(f\"üìä Found {len(data_files)} medical images:\")\n",
    "    for i, file in enumerate(data_files[:5]):  # Show first 5\n",
    "        print(f\"   {i+1}. {os.path.basename(file)}\")\n",
    "    if len(data_files) > 5:\n",
    "        print(f\"   ... and {len(data_files) - 5} more files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63abb757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test data directory if no real data found\n",
    "if USE_SIMULATED_DATA:\n",
    "    print(\"\\nüèóÔ∏è  CREATING TEST DATA DIRECTORY:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    test_data_dir = \"/home/santino/UNC_RAD/test_medical_data\" \n",
    "    os.makedirs(test_data_dir, exist_ok=True)\n",
    "    \n",
    "    # Create some dummy .nii.gz files for testing\n",
    "    try:\n",
    "        import nibabel as nib\n",
    "        import numpy as np\n",
    "        \n",
    "        print(f\"üìÅ Creating test directory: {test_data_dir}\")\n",
    "        \n",
    "        # Create 3 dummy medical images\n",
    "        for i in range(3):\n",
    "            # Create random 3D image data (simulating medical scan)\n",
    "            img_data = np.random.randint(0, 255, (64, 64, 32), dtype=np.uint8)\n",
    "            \n",
    "            # Create NIfTI image\n",
    "            nii_img = nib.Nifti1Image(img_data, affine=np.eye(4))\n",
    "            \n",
    "            # Save as .nii.gz file\n",
    "            filename = f\"test_patient_{i+1:03d}.nii.gz\"\n",
    "            filepath = os.path.join(test_data_dir, filename)\n",
    "            nib.save(nii_img, filepath)\n",
    "            \n",
    "            print(f\"‚úÖ Created: {filename}\")\n",
    "            \n",
    "        # Update paths for testing\n",
    "        REAL_DATA_PATH = test_data_dir\n",
    "        data_files = glob.glob(os.path.join(test_data_dir, \"*.nii.gz\"))\n",
    "        \n",
    "        print(f\"\\nüéâ Test data ready! Created {len(data_files)} files in {test_data_dir}\")\n",
    "        print(\"   You can now run the training with this test data.\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ùå nibabel not available. Cannot create test data.\")\n",
    "        print(\"   Please install nibabel or provide real medical data.\")\n",
    "\n",
    "print(f\"\\nüìã FINAL DATA CONFIGURATION:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Data path: {REAL_DATA_PATH}\")\n",
    "print(f\"Using simulated data: {USE_SIMULATED_DATA}\")\n",
    "print(f\"Number of files: {len(data_files) if 'data_files' in locals() else 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed96d53",
   "metadata": {},
   "source": [
    "# 3. Configure Parallel Training Parameters\n",
    "\n",
    "Now let's configure all the parameters for running the parallel training script. We'll build the exact torchrun command that will work with your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1852bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è  TRAINING CONFIGURATION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check available GPUs again to set nproc_per_node\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        available_gpus = torch.cuda.device_count()\n",
    "        print(f\"Available GPUs: {available_gpus}\")\n",
    "    else:\n",
    "        available_gpus = 1  # Will use CPU\n",
    "        print(\"No GPUs available - will use CPU (very slow)\")\n",
    "except:\n",
    "    available_gpus = 1\n",
    "\n",
    "# Configure training parameters\n",
    "CONFIG = {\n",
    "    # Torchrun parameters - Use actual GPU count, not hardcoded 4\n",
    "    \"nproc_per_node\": available_gpus,  # Use all available GPUs\n",
    "    \"nnodes\": 1,  # Single node for this tutorial\n",
    "    \"master_port\": 29500,  # Use alternative port to avoid conflicts\n",
    "    \n",
    "    # MAISI script parameters  \n",
    "    \"data_path\": REAL_DATA_PATH,\n",
    "    \"use_real_data\": not USE_SIMULATED_DATA,\n",
    "    \"epochs\": 5 if USE_SIMULATED_DATA else 25,  # Short for demo\n",
    "    \"batch_size\": 1,  # Safe default\n",
    "    \"model_version\": \"maisi3d-rflow\",  # Faster version\n",
    "    \"base_seed\": 42,\n",
    "    \n",
    "    # Script location\n",
    "    \"script_path\": \"maisi_train_diff_unet_parallel.py\"\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Build the torchrun command\n",
    "torchrun_cmd = [\n",
    "    \"torchrun\",\n",
    "    f\"--nproc_per_node={CONFIG['nproc_per_node']}\",\n",
    "    f\"--nnodes={CONFIG['nnodes']}\",\n",
    "    f\"--master-port={CONFIG['master_port']}\",  # Add explicit port\n",
    "    CONFIG[\"script_path\"]\n",
    "]\n",
    "\n",
    "# Add MAISI script arguments\n",
    "if CONFIG[\"use_real_data\"]:\n",
    "    torchrun_cmd.extend([\"--real-data\", \"--data-path\", CONFIG[\"data_path\"]])\n",
    "\n",
    "torchrun_cmd.extend([\n",
    "    \"--epochs\", str(CONFIG[\"epochs\"]),\n",
    "    \"--batch-size\", str(CONFIG[\"batch_size\"]),\n",
    "    \"--model-version\", CONFIG[\"model_version\"],\n",
    "    \"--base-seed\", str(CONFIG[\"base_seed\"])\n",
    "])\n",
    "\n",
    "print(f\"\\nüöÄ TORCHRUN COMMAND:\")\n",
    "print(\"=\" * 50)\n",
    "command_str = \" \".join(torchrun_cmd)\n",
    "print(command_str)\n",
    "\n",
    "print(f\"\\nüí° EXPECTED BEHAVIOR:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚Ä¢ Will use {CONFIG['nproc_per_node']} GPU(s) - Perfect for your 2-GPU setup!\")\n",
    "print(f\"‚Ä¢ Will generate {CONFIG['nproc_per_node']} unique medical images\")\n",
    "print(f\"‚Ä¢ Training will run for {CONFIG['epochs']} epochs\")\n",
    "print(f\"‚Ä¢ Each GPU gets batch_size={CONFIG['batch_size']} images\")\n",
    "print(f\"‚Ä¢ Using {'real' if CONFIG['use_real_data'] else 'simulated'} data\")\n",
    "print(f\"‚Ä¢ Using port {CONFIG['master_port']} to avoid conflicts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefcb46a",
   "metadata": {},
   "source": [
    "# 4. Run MAISI Parallel Training Script\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT**: Make sure the `maisi_train_diff_unet_parallel.py` script is in your current directory before running this cell!\n",
    "\n",
    "This cell will execute the training and capture all output in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d91a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üöÄ STARTING MAISI PARALLEL TRAINING:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if script exists\n",
    "script_path = CONFIG[\"script_path\"]\n",
    "if not os.path.exists(script_path):\n",
    "    print(f\"‚ùå ERROR: Script not found: {script_path}\")\n",
    "    print(f\"   Please make sure {script_path} is in your current directory:\")\n",
    "    print(f\"   Current directory: {os.getcwd()}\")\n",
    "    print(f\"   Files in directory: {os.listdir('.')}\")\n",
    "    print(f\"\\n   You can download it from the MONAI repository or copy it to this location.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found script: {script_path}\")\n",
    "    \n",
    "    # Show the command we're about to run\n",
    "    print(f\"\\nüìã Command to execute:\")\n",
    "    print(\" \".join(torchrun_cmd))\n",
    "    \n",
    "    print(f\"\\n‚è∞ Training started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Set environment variable to avoid OMP warnings\n",
    "        env = os.environ.copy()\n",
    "        env[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "        \n",
    "        # Start the process\n",
    "        process = subprocess.Popen(\n",
    "            torchrun_cmd,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,  # Combine stderr with stdout\n",
    "            text=True,\n",
    "            bufsize=1,  # Line buffered\n",
    "            universal_newlines=True,\n",
    "            env=env\n",
    "        )\n",
    "        \n",
    "        # Read output in real-time\n",
    "        output_lines = []\n",
    "        for line in process.stdout:\n",
    "            print(line, end='')  # Print immediately \n",
    "            output_lines.append(line)\n",
    "            \n",
    "        # Wait for process to complete\n",
    "        return_code = process.wait()\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(f\"‚è∞ Training completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        if return_code == 0:\n",
    "            print(\"üéâ SUCCESS: Training completed successfully!\")\n",
    "        else:\n",
    "            print(f\"‚ùå FAILED: Training failed with return code {return_code}\")\n",
    "            \n",
    "        # Store output for analysis\n",
    "        training_output = \"\".join(output_lines)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå ERROR: torchrun not found. Please install PyTorch properly.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR: Unexpected error during training: {e}\")\n",
    "        training_output = str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3e42e",
   "metadata": {},
   "source": [
    "# 5. Monitor Output and Handle Errors\n",
    "\n",
    "Let's analyze the training output and provide guidance if any errors occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea1a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç OUTPUT ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'training_output' in locals():\n",
    "    # Check for common error patterns\n",
    "    error_patterns = {\n",
    "        \"No .nii.gz files found\": \"Data path issue - no medical images found\",\n",
    "        \"CUDA out of memory\": \"GPU memory insufficient - reduce batch size\",\n",
    "        \"ConnectionTimeout\": \"Multi-node communication issue\",\n",
    "        \"FileNotFoundError\": \"Missing file or script\",\n",
    "        \"ImportError\": \"Missing Python package\",\n",
    "        \"Permission denied\": \"File permission issue\",\n",
    "        \"Address already in use\": \"Port conflict in multi-node setup\"\n",
    "    }\n",
    "    \n",
    "    errors_found = []\n",
    "    for pattern, description in error_patterns.items():\n",
    "        if pattern.lower() in training_output.lower():\n",
    "            errors_found.append((pattern, description))\n",
    "    \n",
    "    if errors_found:\n",
    "        print(\"‚ùå ERRORS DETECTED:\")\n",
    "        for pattern, description in errors_found:\n",
    "            print(f\"   ‚Ä¢ {pattern}: {description}\")\n",
    "            \n",
    "        print(f\"\\nüîß TROUBLESHOOTING GUIDE:\")\n",
    "        \n",
    "        if \"No .nii.gz files found\" in [p for p, d in errors_found]:\n",
    "            print(f\"üìÅ Data Path Issue:\")\n",
    "            print(f\"   ‚Ä¢ Check that your data path exists: {CONFIG['data_path']}\")\n",
    "            print(f\"   ‚Ä¢ Verify .nii.gz files are present: ls {CONFIG['data_path']}/*.nii.gz\")\n",
    "            print(f\"   ‚Ä¢ Update REAL_DATA_PATH variable in cell 2\")\n",
    "            print(f\"   ‚Ä¢ Or use simulated data for testing\")\n",
    "            \n",
    "        if \"CUDA out of memory\" in [p for p, d in errors_found]:\n",
    "            print(f\"üíæ Memory Issue:\")\n",
    "            print(f\"   ‚Ä¢ Reduce batch_size to 1: --batch-size 1\")\n",
    "            print(f\"   ‚Ä¢ Use fewer GPUs: --nproc_per_node 1\")  \n",
    "            print(f\"   ‚Ä¢ Close other GPU applications\")\n",
    "            \n",
    "    else:\n",
    "        print(\"‚úÖ No obvious errors detected in output\")\n",
    "        \n",
    "    # Check for success indicators\n",
    "    success_patterns = [\n",
    "        \"Training completed successfully\",\n",
    "        \"inference completed\",\n",
    "        \"Generated images saved\",\n",
    "        \"MAISI PARALLEL COMPUTING CONFIGURATION\"\n",
    "    ]\n",
    "    \n",
    "    success_found = any(pattern.lower() in training_output.lower() \n",
    "                       for pattern in success_patterns)\n",
    "    \n",
    "    if success_found:\n",
    "        print(\"üéâ SUCCESS INDICATORS FOUND:\")\n",
    "        for pattern in success_patterns:\n",
    "            if pattern.lower() in training_output.lower():\n",
    "                print(f\"   ‚úÖ {pattern}\")\n",
    "    \n",
    "    # Extract key information from output\n",
    "    lines = training_output.split('\\n')\n",
    "    config_lines = [line for line in lines if '‚Ä¢' in line and any(\n",
    "        keyword in line.lower() for keyword in \n",
    "        ['gpus', 'epochs', 'batch size', 'data path', 'model version']\n",
    "    )]\n",
    "    \n",
    "    if config_lines:\n",
    "        print(f\"\\nüìä EXTRACTED CONFIGURATION:\")\n",
    "        for line in config_lines[:10]:  # Show first 10 config lines\n",
    "            print(f\"   {line.strip()}\")\n",
    "            \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No training output available to analyze\")\n",
    "    print(\"   Make sure to run the training cell above first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef738310",
   "metadata": {},
   "source": [
    "# 6. Inspect Generated Images and Output Structure\n",
    "\n",
    "After successful training, let's examine the output directory structure and generated medical images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa55b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üìÇ OUTPUT DIRECTORY INSPECTION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check for the typical output directory\n",
    "output_dir = \"./output_work_dir\"\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    print(f\"‚úÖ Found output directory: {output_dir}\")\n",
    "    \n",
    "    # Show directory structure\n",
    "    print(f\"\\nüìÅ Directory Structure:\")\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        level = root.replace(output_dir, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files[:5]:  # Show first 5 files per directory\n",
    "            print(f\"{subindent}{file}\")\n",
    "        if len(files) > 5:\n",
    "            print(f\"{subindent}... and {len(files) - 5} more files\")\n",
    "    \n",
    "    # Look for generated images\n",
    "    generated_images = glob.glob(os.path.join(output_dir, \"**\", \"*.nii.gz\"), recursive=True)\n",
    "    \n",
    "    if generated_images:\n",
    "        print(f\"\\nüñºÔ∏è  GENERATED MEDICAL IMAGES:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Found {len(generated_images)} generated images:\")\n",
    "        \n",
    "        for img_path in generated_images:\n",
    "            rel_path = os.path.relpath(img_path, output_dir)\n",
    "            file_size = os.path.getsize(img_path) / 1024**2  # MB\n",
    "            print(f\"   üìÑ {rel_path} ({file_size:.1f} MB)\")\n",
    "            \n",
    "        # Analyze naming pattern\n",
    "        output_images = [img for img in generated_images if 'output' in img or 'generated' in img]\n",
    "        if output_images:\n",
    "            print(f\"\\nüéØ Generated Images (Final Results):\")\n",
    "            for img in output_images:\n",
    "                filename = os.path.basename(img)\n",
    "                print(f\"   üñºÔ∏è  {filename}\")\n",
    "                \n",
    "                # Try to extract seed and rank from filename\n",
    "                if 'seed' in filename and 'rank' in filename:\n",
    "                    try:\n",
    "                        parts = filename.replace('.nii.gz', '').split('_')\n",
    "                        seed_part = [p for p in parts if p.startswith('seed')]\n",
    "                        rank_part = [p for p in parts if p.startswith('rank')]\n",
    "                        if seed_part and rank_part:\n",
    "                            seed = seed_part[0].replace('seed', '')\n",
    "                            rank = rank_part[0].replace('rank', '')\n",
    "                            print(f\"      ‚Ä¢ GPU {rank} generated this with seed {seed}\")\n",
    "                    except:\n",
    "                        pass\n",
    "                        \n",
    "        print(f\"\\nüí° VIEWING YOUR RESULTS:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"You can view these medical images with:\")\n",
    "        print(\"‚Ä¢ 3D Slicer (recommended): https://www.slicer.org/\")\n",
    "        print(\"‚Ä¢ ITK-SNAP: http://www.itksnap.org/\")\n",
    "        print(\"‚Ä¢ FSL tools: fsleyes your_image.nii.gz\")\n",
    "        print(\"‚Ä¢ Python/MONAI: nibabel.load('your_image.nii.gz')\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No generated .nii.gz images found\")\n",
    "        print(\"   Training may not have completed successfully\")\n",
    "        \n",
    "    # Check for model checkpoints\n",
    "    model_files = glob.glob(os.path.join(output_dir, \"**\", \"*.pt\"), recursive=True)\n",
    "    if model_files:\n",
    "        print(f\"\\nü§ñ MODEL CHECKPOINTS:\")\n",
    "        print(\"=\" * 50)\n",
    "        for model_path in model_files:\n",
    "            rel_path = os.path.relpath(model_path, output_dir)\n",
    "            file_size = os.path.getsize(model_path) / 1024**2  # MB\n",
    "            print(f\"   üíæ {rel_path} ({file_size:.1f} MB)\")\n",
    "            \n",
    "else:\n",
    "    print(f\"‚ùå Output directory not found: {output_dir}\")\n",
    "    print(\"   Training may not have started or failed early\")\n",
    "    print(\"   Check the training output above for errors\")\n",
    "\n",
    "print(f\"\\nüìä SUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "if os.path.exists(output_dir):\n",
    "    total_size = sum(os.path.getsize(os.path.join(dirpath, filename))\n",
    "                    for dirpath, dirnames, filenames in os.walk(output_dir)\n",
    "                    for filename in filenames) / 1024**2\n",
    "    print(f\"‚Ä¢ Total output size: {total_size:.1f} MB\")\n",
    "    print(f\"‚Ä¢ Output location: {os.path.abspath(output_dir)}\")\n",
    "else:\n",
    "    print(\"‚Ä¢ No output generated yet\")\n",
    "    print(\"‚Ä¢ Run the training script successfully first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c43a6f7",
   "metadata": {},
   "source": [
    "# üéâ Tutorial Complete!\n",
    "\n",
    "## What We Accomplished\n",
    "\n",
    "‚úÖ **Environment Setup**: Verified Python, installed dependencies, checked GPU availability  \n",
    "‚úÖ **Data Configuration**: Set up proper data paths and created test data if needed  \n",
    "‚úÖ **Parameter Configuration**: Built the correct torchrun command with all arguments  \n",
    "‚úÖ **Training Execution**: Ran the parallel training script with proper error handling  \n",
    "‚úÖ **Output Analysis**: Examined generated images and directory structure  \n",
    "\n",
    "## Your Error Resolution\n",
    "\n",
    "The original error `ValueError: No .nii.gz files found in /path/to/medical/data` occurred because:\n",
    "- `/path/to/medical/data` was just a placeholder path\n",
    "- The script couldn't find any medical imaging files\n",
    "\n",
    "**We fixed this by:**\n",
    "1. Automatically searching common data directories\n",
    "2. Creating test data if no real data was found\n",
    "3. Using the correct data path in the torchrun command\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### For Better Results:\n",
    "1. **Use Real Medical Data**: Replace test data with actual CT/MRI scans\n",
    "2. **Increase Training**: Use 100+ epochs for production-quality results\n",
    "3. **Scale Up**: Use more GPUs and larger datasets\n",
    "4. **Optimize**: Tune batch size and model parameters\n",
    "\n",
    "### Command Templates:\n",
    "```bash\n",
    "# Quick test (what we just ran)\n",
    "torchrun --nproc_per_node=4 maisi_train_diff_unet_parallel.py --real-data --data-path /your/data/path --epochs 5\n",
    "\n",
    "# Production training\n",
    "torchrun --nproc_per_node=8 maisi_train_diff_unet_parallel.py --real-data --data-path /your/data/path --epochs 100 --batch-size 2\n",
    "\n",
    "# Multi-node cluster\n",
    "torchrun --nproc_per_node=4 --nnodes=2 --node_rank=0 --master_addr=192.168.1.100 --master_port=29500 maisi_train_diff_unet_parallel.py --real-data --data-path /shared/data\n",
    "```\n",
    "\n",
    "### Viewing Results:\n",
    "- **3D Slicer**: Best for medical imaging visualization\n",
    "- **ITK-SNAP**: Alternative medical image viewer  \n",
    "- **Python**: Use `nibabel.load()` for programmatic access\n",
    "\n",
    "**Happy Training! üöÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97f94c1",
   "metadata": {},
   "source": [
    "# üö® Troubleshooting: Parallel Training Errors\n",
    "\n",
    "Based on your output, I can see two main issues that need to be fixed before running the parallel training successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3711a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import socket\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "print(\"üîç DIAGNOSING PARALLEL TRAINING ISSUES:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Issue 1: Check actual GPU count\n",
    "print(\"1Ô∏è‚É£ GPU COUNT CHECK:\")\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        actual_gpu_count = torch.cuda.device_count()\n",
    "        print(f\"   ‚úÖ Actual GPUs available: {actual_gpu_count}\")\n",
    "        for i in range(actual_gpu_count):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            print(f\"      GPU {i}: {gpu_name}\")\n",
    "    else:\n",
    "        actual_gpu_count = 0\n",
    "        print(\"   ‚ùå No CUDA GPUs available\")\n",
    "except ImportError:\n",
    "    actual_gpu_count = 0\n",
    "    print(\"   ‚ùå PyTorch not available\")\n",
    "\n",
    "# Issue 2: Check port usage\n",
    "print(f\"\\n2Ô∏è‚É£ PORT CONFLICT CHECK:\")\n",
    "def check_port(port):\n",
    "    \"\"\"Check if a port is in use.\"\"\"\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            result = s.connect_ex(('localhost', port))\n",
    "            return result == 0  # 0 means connection successful (port in use)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "ports_to_check = [12355, 29500, 29501, 23456, 25000]\n",
    "available_port = None\n",
    "\n",
    "for port in ports_to_check:\n",
    "    if check_port(port):\n",
    "        print(f\"   ‚ùå Port {port}: IN USE\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Port {port}: Available\")\n",
    "        if available_port is None:\n",
    "            available_port = port\n",
    "\n",
    "# Issue 3: Check for running torchrun processes\n",
    "print(f\"\\n3Ô∏è‚É£ RUNNING PROCESSES CHECK:\")\n",
    "try:\n",
    "    running_torchrun = []\n",
    "    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
    "        try:\n",
    "            if 'torchrun' in ' '.join(proc.info['cmdline'] or []):\n",
    "                running_torchrun.append(proc.info)\n",
    "        except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "            pass\n",
    "    \n",
    "    if running_torchrun:\n",
    "        print(f\"   ‚ö†Ô∏è  Found {len(running_torchrun)} running torchrun processes:\")\n",
    "        for proc in running_torchrun:\n",
    "            print(f\"      PID {proc['pid']}: {' '.join(proc['cmdline'][:3])}\")\n",
    "        print(f\"   üí° These may be causing port conflicts\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ No conflicting torchrun processes found\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(f\"   ‚ö†Ô∏è  psutil not available for process check\")\n",
    "\n",
    "print(f\"\\nüîß RECOMMENDED FIXES:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if actual_gpu_count < 4:\n",
    "    print(f\"‚ùó GPU COUNT MISMATCH:\")\n",
    "    print(f\"   ‚Ä¢ You have {actual_gpu_count} GPUs but script tried to use 4\")\n",
    "    print(f\"   ‚Ä¢ SOLUTION: Use --nproc_per_node={actual_gpu_count} instead of 4\")\n",
    "    recommended_gpus = actual_gpu_count\n",
    "else:\n",
    "    recommended_gpus = 4\n",
    "\n",
    "if available_port != 12355:\n",
    "    print(f\"‚ùó PORT CONFLICT:\")\n",
    "    print(f\"   ‚Ä¢ Port 12355 is in use\")\n",
    "    print(f\"   ‚Ä¢ SOLUTION: Use --master-port {available_port}\")\n",
    "    recommended_port = available_port\n",
    "else:\n",
    "    recommended_port = 12355\n",
    "\n",
    "print(f\"\\nüöÄ CORRECTED COMMAND:\")\n",
    "print(\"=\" * 60)\n",
    "corrected_command = f\"torchrun --nproc_per_node={recommended_gpus} --master-port {recommended_port} maisi_train_diff_unet_parallel.py\"\n",
    "print(corrected_command)\n",
    "\n",
    "print(f\"\\nüí° ADDITIONAL TIPS:\")\n",
    "print(\"‚Ä¢ Kill existing processes: pkill -f torchrun\")\n",
    "print(\"‚Ä¢ Wait a few seconds between runs for ports to be released\")\n",
    "print(\"‚Ä¢ Use different terminal/session if issues persist\")\n",
    "print(\"‚Ä¢ Monitor with: nvidia-smi (for GPUs) and netstat -tulpn | grep :12355 (for ports)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f746e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any existing processes and run with correct parameters\n",
    "print(\"üßπ CLEANUP AND CORRECTED EXECUTION:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Step 1: Kill any existing torchrun processes\n",
    "print(\"1Ô∏è‚É£ Cleaning up existing processes...\")\n",
    "try:\n",
    "    subprocess.run([\"pkill\", \"-f\", \"torchrun\"], capture_output=True)\n",
    "    print(\"   ‚úÖ Killed existing torchrun processes\")\n",
    "    time.sleep(3)  # Wait for cleanup\n",
    "except:\n",
    "    print(\"   ‚ö†Ô∏è  Could not kill processes (may not exist)\")\n",
    "\n",
    "# Step 2: Build corrected command\n",
    "if 'recommended_gpus' in locals() and 'recommended_port' in locals():\n",
    "    corrected_cmd = [\n",
    "        \"torchrun\",\n",
    "        f\"--nproc_per_node={recommended_gpus}\",\n",
    "        f\"--master-port={recommended_port}\",\n",
    "        \"maisi_train_diff_unet_parallel.py\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"2Ô∏è‚É£ Running corrected command:\")\n",
    "    print(f\"   {' '.join(corrected_cmd)}\")\n",
    "    \n",
    "    # Step 3: Execute the corrected command\n",
    "    try:\n",
    "        env = os.environ.copy()\n",
    "        env[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "        \n",
    "        print(f\"\\n‚è∞ Started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        process = subprocess.Popen(\n",
    "            corrected_cmd,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True,\n",
    "            env=env\n",
    "        )\n",
    "        \n",
    "        # Monitor output\n",
    "        output_lines = []\n",
    "        for line in process.stdout:\n",
    "            print(line, end='')\n",
    "            output_lines.append(line)\n",
    "            \n",
    "        return_code = process.wait()\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(f\"‚è∞ Completed at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        if return_code == 0:\n",
    "            print(\"üéâ SUCCESS: Parallel training completed!\")\n",
    "        else:\n",
    "            print(f\"‚ùå FAILED: Return code {return_code}\")\n",
    "            \n",
    "        training_output_corrected = \"\".join(output_lines)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR: {e}\")\n",
    "        training_output_corrected = str(e)\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Could not determine correct parameters. Run diagnosis cell above first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488744a0",
   "metadata": {},
   "source": [
    "# üéØ Quick Fix - Manual Commands\n",
    "\n",
    "If you want to fix this immediately in your terminal, here are the exact commands to run:\n",
    "\n",
    "## Problem Analysis:\n",
    "1. **GPU Mismatch**: You have 2 GPUs but the script tried to use 4\n",
    "2. **Port Conflict**: Multiple processes trying to use port 12355\n",
    "\n",
    "## Immediate Solutions:\n",
    "\n",
    "### Option 1: Use 2 GPUs (Recommended)\n",
    "```bash\n",
    "# Kill any existing processes\n",
    "pkill -f torchrun\n",
    "\n",
    "# Wait a moment\n",
    "sleep 3\n",
    "\n",
    "# Run with correct GPU count\n",
    "torchrun --nproc_per_node=2 maisi_train_diff_unet_parallel.py\n",
    "```\n",
    "\n",
    "### Option 2: Use different port\n",
    "```bash\n",
    "# Kill existing processes  \n",
    "pkill -f torchrun\n",
    "\n",
    "# Run with different port\n",
    "torchrun --nproc_per_node=2 --master-port=29500 maisi_train_diff_unet_parallel.py\n",
    "```\n",
    "\n",
    "### Option 3: Single GPU (Most Reliable)\n",
    "```bash\n",
    "# Use single GPU to avoid distributed issues\n",
    "python maisi_train_diff_unet.py\n",
    "```\n",
    "\n",
    "## Why This Happened:\n",
    "- Your system has 2 RTX 4090 GPUs\n",
    "- The parallel script defaulted to 4 GPUs  \n",
    "- Multiple torchrun processes created port conflicts\n",
    "- GPU ranks 2 and 3 don't exist on your system\n",
    "\n",
    "**Recommendation**: Use Option 1 with 2 GPUs for optimal performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e3c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ AUTOMATIC FIX: Run with Your 2 GPUs\n",
    "print(\"üéØ RUNNING MAISI PARALLEL TRAINING WITH 2 GPUS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Step 1: Clean up any existing processes\n",
    "print(\"1Ô∏è‚É£ Cleaning up existing processes...\")\n",
    "try:\n",
    "    subprocess.run([\"pkill\", \"-f\", \"torchrun\"], capture_output=True, timeout=10)\n",
    "    print(\"   ‚úÖ Cleaned up existing torchrun processes\")\n",
    "    time.sleep(5)  # Give more time for cleanup\n",
    "except:\n",
    "    print(\"   ‚ö†Ô∏è  No existing processes to clean (this is fine)\")\n",
    "\n",
    "# Step 2: Verify your 2 GPUs\n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"\\n2Ô∏è‚É£ Confirmed: You have {gpu_count} GPUs available\")\n",
    "        for i in range(gpu_count):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "            print(f\"   GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "    else:\n",
    "        print(\"‚ùå No GPUs detected\")\n",
    "        gpu_count = 0\n",
    "except ImportError:\n",
    "    print(\"‚ùå PyTorch not available\")\n",
    "    gpu_count = 0\n",
    "\n",
    "if gpu_count >= 2:\n",
    "    # Step 3: Run with exactly 2 GPUs and available port\n",
    "    available_port = 29500  # Use different port to avoid conflicts\n",
    "    \n",
    "    corrected_cmd = [\n",
    "        \"torchrun\",\n",
    "        f\"--nproc_per_node={gpu_count}\",  # Use actual GPU count\n",
    "        \"--nnodes=1\",\n",
    "        f\"--master-port={available_port}\",  # Use different port\n",
    "        \"maisi_train_diff_unet_parallel.py\",\n",
    "        \"--epochs\", \"5\",  # Short run for testing\n",
    "        \"--batch-size\", \"1\",\n",
    "        \"--model-version\", \"maisi3d-rflow\",\n",
    "        \"--base-seed\", \"42\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n3Ô∏è‚É£ Running optimized command for your 2-GPU system:\")\n",
    "    print(f\"   {' '.join(corrected_cmd)}\")\n",
    "    print(f\"\\nüí° This will:\")\n",
    "    print(f\"   ‚Ä¢ Use both of your RTX 4090 GPUs\")\n",
    "    print(f\"   ‚Ä¢ Generate 2 unique medical images simultaneously\")\n",
    "    print(f\"   ‚Ä¢ Use simulated data (quick demo)\")\n",
    "    print(f\"   ‚Ä¢ Avoid port conflicts with port {available_port}\")\n",
    "    \n",
    "    print(f\"\\n‚è∞ Training started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"üöÄ \" + \"=\" * 58)\n",
    "    \n",
    "    try:\n",
    "        # Set up environment\n",
    "        env = os.environ.copy()\n",
    "        env[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "        env[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Explicitly set 2 GPUs\n",
    "        \n",
    "        # Start the corrected process\n",
    "        process = subprocess.Popen(\n",
    "            corrected_cmd,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True,\n",
    "            env=env,\n",
    "            cwd=os.getcwd()\n",
    "        )\n",
    "        \n",
    "        # Monitor output in real-time\n",
    "        output_lines = []\n",
    "        for line in process.stdout:\n",
    "            print(line, end='')\n",
    "            output_lines.append(line)\n",
    "            \n",
    "        # Wait for completion\n",
    "        return_code = process.wait()\n",
    "        \n",
    "        print(\"üèÅ \" + \"=\" * 58)\n",
    "        print(f\"‚è∞ Training completed at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        if return_code == 0:\n",
    "            print(\"üéâ SUCCESS: 2-GPU parallel training completed successfully!\")\n",
    "            print(\"üìÅ Check the output_work_dir for your generated medical images!\")\n",
    "        else:\n",
    "            print(f\"‚ùå Training failed with return code: {return_code}\")\n",
    "            \n",
    "        # Store output for later analysis\n",
    "        training_output_final = \"\".join(output_lines)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR during execution: {e}\")\n",
    "        training_output_final = str(e)\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Cannot run parallel training - need at least 2 GPUs but found {gpu_count}\")\n",
    "    print(\"üí° Try the single-GPU version instead: python maisi_train_diff_unet.py\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
